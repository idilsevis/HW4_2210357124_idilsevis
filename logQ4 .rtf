{\rtf1\ansi\ansicpg1252\cocoartf2822
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 ==================================================\
BASELINE EXPERIMENT\
==================================================\
Baseline Epoch 1, Batch 0, Loss: 2.284752\
Baseline Epoch 1, Batch 200, Loss: 0.193543\
Baseline Epoch 1, Batch 400, Loss: 0.163851\
Baseline Epoch 1, Batch 600, Loss: 0.121924\
Baseline Epoch 1, Batch 800, Loss: 0.046698\
Baseline Epoch 1: Train Acc: 93.74%, Test Acc: 98.16%, Loss: 0.207777\
Baseline Epoch 2, Batch 0, Loss: 0.054073\
Baseline Epoch 2, Batch 200, Loss: 0.020730\
Baseline Epoch 2, Batch 400, Loss: 0.071031\
Baseline Epoch 2, Batch 600, Loss: 0.049537\
Baseline Epoch 2, Batch 800, Loss: 0.006853\
Baseline Epoch 2: Train Acc: 97.67%, Test Acc: 98.82%, Loss: 0.078802\
Baseline Epoch 3, Batch 0, Loss: 0.052613\
Baseline Epoch 3, Batch 200, Loss: 0.042539\
Baseline Epoch 3, Batch 400, Loss: 0.005639\
Baseline Epoch 3, Batch 600, Loss: 0.062199\
Baseline Epoch 3, Batch 800, Loss: 0.073429\
Baseline Epoch 3: Train Acc: 98.27%, Test Acc: 99.04%, Loss: 0.058952\
Baseline Epoch 4, Batch 0, Loss: 0.022790\
Baseline Epoch 4, Batch 200, Loss: 0.010736\
Baseline Epoch 4, Batch 400, Loss: 0.084703\
Baseline Epoch 4, Batch 600, Loss: 0.033784\
Baseline Epoch 4, Batch 800, Loss: 0.047802\
Baseline Epoch 4: Train Acc: 98.52%, Test Acc: 99.14%, Loss: 0.048769\
Baseline Epoch 5, Batch 0, Loss: 0.069781\
Baseline Epoch 5, Batch 200, Loss: 0.014847\
Baseline Epoch 5, Batch 400, Loss: 0.011035\
Baseline Epoch 5, Batch 600, Loss: 0.047466\
Baseline Epoch 5, Batch 800, Loss: 0.065112\
Baseline Epoch 5: Train Acc: 98.82%, Test Acc: 99.13%, Loss: 0.039285\
Baseline Epoch 6, Batch 0, Loss: 0.003007\
Baseline Epoch 6, Batch 200, Loss: 0.044698\
Baseline Epoch 6, Batch 400, Loss: 0.012262\
Baseline Epoch 6, Batch 600, Loss: 0.012965\
Baseline Epoch 6, Batch 800, Loss: 0.005341\
Baseline Epoch 6: Train Acc: 98.92%, Test Acc: 99.15%, Loss: 0.034043\
Baseline Epoch 7, Batch 0, Loss: 0.053850\
Baseline Epoch 7, Batch 200, Loss: 0.070365\
Baseline Epoch 7, Batch 400, Loss: 0.022921\
Baseline Epoch 7, Batch 600, Loss: 0.002263\
Baseline Epoch 7, Batch 800, Loss: 0.029168\
Baseline Epoch 7: Train Acc: 99.11%, Test Acc: 99.12%, Loss: 0.028735\
Baseline Epoch 8, Batch 0, Loss: 0.003922\
Baseline Epoch 8, Batch 200, Loss: 0.008347\
Baseline Epoch 8, Batch 400, Loss: 0.095229\
Baseline Epoch 8, Batch 600, Loss: 0.090005\
Baseline Epoch 8, Batch 800, Loss: 0.011564\
Baseline Epoch 8: Train Acc: 99.19%, Test Acc: 99.30%, Loss: 0.026599\
Baseline Epoch 9, Batch 0, Loss: 0.004893\
Baseline Epoch 9, Batch 200, Loss: 0.004503\
Baseline Epoch 9, Batch 400, Loss: 0.002315\
Baseline Epoch 9, Batch 600, Loss: 0.039986\
Baseline Epoch 9, Batch 800, Loss: 0.005859\
Baseline Epoch 9: Train Acc: 99.25%, Test Acc: 99.34%, Loss: 0.025105\
Baseline Epoch 10, Batch 0, Loss: 0.016536\
Baseline Epoch 10, Batch 200, Loss: 0.003584\
Baseline Epoch 10, Batch 400, Loss: 0.054169\
Baseline Epoch 10, Batch 600, Loss: 0.009066\
Baseline Epoch 10, Batch 800, Loss: 0.042287\
Baseline Epoch 10: Train Acc: 99.30%, Test Acc: 99.33%, Loss: 0.021740\
==================================================\
FAILED EXPERIMENT 1: EXTREME LEARNING RATE\
==================================================\
Using learning rate of 10.0 - this will cause gradient explosion\
Failed-1 (Extreme LR) Epoch 1, Batch 0, Loss: 2.307539\
Failed-1 (Extreme LR) Epoch 1, Batch 200, Loss: 2.619227\
Failed-1 (Extreme LR) Epoch 1, Batch 400, Loss: 2.697615\
Failed-1 (Extreme LR) Epoch 1, Batch 600, Loss: 3.105114\
Failed-1 (Extreme LR) Epoch 1, Batch 800, Loss: 2.703813\
Failed-1 (Extreme LR) Epoch 1: Train Acc: 10.04%, Test Acc: 10.32%, Loss: 124551851.743536\
Failed-1 (Extreme LR) Epoch 2, Batch 0, Loss: 2.651330\
Failed-1 (Extreme LR) Epoch 2, Batch 200, Loss: 3.420753\
Failed-1 (Extreme LR) Epoch 2, Batch 400, Loss: 2.562241\
Failed-1 (Extreme LR) Epoch 2, Batch 600, Loss: 2.469597\
Failed-1 (Extreme LR) Epoch 2, Batch 800, Loss: 2.670556\
Failed-1 (Extreme LR) Epoch 2: Train Acc: 10.08%, Test Acc: 9.82%, Loss: 2.881302\
Failed-1 (Extreme LR) Epoch 3, Batch 0, Loss: 3.260110\
Failed-1 (Extreme LR) Epoch 3, Batch 200, Loss: 3.055355\
Failed-1 (Extreme LR) Epoch 3, Batch 400, Loss: 2.687711\
Failed-1 (Extreme LR) Epoch 3, Batch 600, Loss: 3.194034\
Failed-1 (Extreme LR) Epoch 3, Batch 800, Loss: 3.679595\
Failed-1 (Extreme LR) Epoch 3: Train Acc: 10.06%, Test Acc: 9.74%, Loss: 2.964145\
Failed-1 (Extreme LR) Epoch 4, Batch 0, Loss: 2.990063\
Failed-1 (Extreme LR) Epoch 4, Batch 200, Loss: 2.977101\
Failed-1 (Extreme LR) Epoch 4, Batch 400, Loss: 4.691300\
Failed-1 (Extreme LR) Epoch 4, Batch 600, Loss: 2.909801\
Failed-1 (Extreme LR) Epoch 4, Batch 800, Loss: 2.872387\
Failed-1 (Extreme LR) Epoch 4: Train Acc: 10.01%, Test Acc: 11.35%, Loss: 2.999609\
Failed-1 (Extreme LR) Epoch 5, Batch 0, Loss: 3.457964\
Failed-1 (Extreme LR) Epoch 5, Batch 200, Loss: 2.830751\
Failed-1 (Extreme LR) Epoch 5, Batch 400, Loss: 3.314516\
Failed-1 (Extreme LR) Epoch 5, Batch 600, Loss: 3.205984\
Failed-1 (Extreme LR) Epoch 5, Batch 800, Loss: 3.268727\
Failed-1 (Extreme LR) Epoch 5: Train Acc: 10.09%, Test Acc: 10.09%, Loss: 2.902734\
Failed-1 (Extreme LR) Epoch 6, Batch 0, Loss: 3.526593\
Failed-1 (Extreme LR) Epoch 6, Batch 200, Loss: 2.871752\
Failed-1 (Extreme LR) Epoch 6, Batch 400, Loss: 2.806316\
Failed-1 (Extreme LR) Epoch 6, Batch 600, Loss: 2.350678\
Failed-1 (Extreme LR) Epoch 6, Batch 800, Loss: 2.756649\
Failed-1 (Extreme LR) Epoch 6: Train Acc: 10.09%, Test Acc: 9.80%, Loss: 2.933143\
Failed-1 (Extreme LR) Epoch 7, Batch 0, Loss: 2.904606\
Failed-1 (Extreme LR) Epoch 7, Batch 200, Loss: 2.658627\
Failed-1 (Extreme LR) Epoch 7, Batch 400, Loss: 2.542441\
Failed-1 (Extreme LR) Epoch 7, Batch 600, Loss: 2.420289\
Failed-1 (Extreme LR) Epoch 7, Batch 800, Loss: 2.958655\
Failed-1 (Extreme LR) Epoch 7: Train Acc: 10.12%, Test Acc: 9.80%, Loss: 2.939110\
Failed-1 (Extreme LR) Epoch 8, Batch 0, Loss: 3.252473\
Failed-1 (Extreme LR) Epoch 8, Batch 200, Loss: 2.806108\
Failed-1 (Extreme LR) Epoch 8, Batch 400, Loss: 2.854528\
Failed-1 (Extreme LR) Epoch 8, Batch 600, Loss: 2.878725\
Failed-1 (Extreme LR) Epoch 8, Batch 800, Loss: 4.366742\
Failed-1 (Extreme LR) Epoch 8: Train Acc: 10.05%, Test Acc: 9.80%, Loss: 3.008856\
Failed-1 (Extreme LR) Epoch 9, Batch 0, Loss: 2.689013\
Failed-1 (Extreme LR) Epoch 9, Batch 200, Loss: 3.234032\
Failed-1 (Extreme LR) Epoch 9, Batch 400, Loss: 2.865057\
Failed-1 (Extreme LR) Epoch 9, Batch 600, Loss: 2.579389\
Failed-1 (Extreme LR) Epoch 9, Batch 800, Loss: 2.701828\
Failed-1 (Extreme LR) Epoch 9: Train Acc: 10.08%, Test Acc: 10.32%, Loss: 2.929469\
Failed-1 (Extreme LR) Epoch 10, Batch 0, Loss: 3.307899\
Failed-1 (Extreme LR) Epoch 10, Batch 200, Loss: 3.127316\
Failed-1 (Extreme LR) Epoch 10, Batch 400, Loss: 2.672248\
Failed-1 (Extreme LR) Epoch 10, Batch 600, Loss: 3.006515\
Failed-1 (Extreme LR) Epoch 10, Batch 800, Loss: 3.547761\
Failed-1 (Extreme LR) Epoch 10: Train Acc: 10.08%, Test Acc: 8.92%, Loss: 2.931625\
==================================================\
FAILED EXPERIMENT 2: MASSIVE OVERPARAMETERIZATION\
==================================================\
Using huge network with tiny dataset (500 samples) - severe overfitting expected\
Failed-2 (Overfit) Epoch 1, Batch 0, Loss: 2.307937\
Failed-2 (Overfit) Epoch 1: Train Acc: 10.00%, Test Acc: 10.09%, Loss: 2.984858\
Failed-2 (Overfit) Epoch 2, Batch 0, Loss: 2.294994\
Failed-2 (Overfit) Epoch 2: Train Acc: 10.00%, Test Acc: 10.28%, Loss: 2.310277\
Failed-2 (Overfit) Epoch 3, Batch 0, Loss: 2.293869\
Failed-2 (Overfit) Epoch 3: Train Acc: 12.40%, Test Acc: 11.35%, Loss: 2.297607\
Failed-2 (Overfit) Epoch 4, Batch 0, Loss: 2.297733\
Failed-2 (Overfit) Epoch 4: Train Acc: 13.20%, Test Acc: 11.35%, Loss: 2.298923\
Failed-2 (Overfit) Epoch 5, Batch 0, Loss: 2.296139\
Failed-2 (Overfit) Epoch 5: Train Acc: 13.20%, Test Acc: 11.35%, Loss: 2.295448\
Failed-2 (Overfit) Epoch 6, Batch 0, Loss: 2.290239\
Failed-2 (Overfit) Epoch 6: Train Acc: 13.20%, Test Acc: 11.35%, Loss: 2.272690\
Failed-2 (Overfit) Epoch 7, Batch 0, Loss: 2.202507\
Failed-2 (Overfit) Epoch 7: Train Acc: 18.20%, Test Acc: 19.54%, Loss: 2.073875\
Failed-2 (Overfit) Epoch 8, Batch 0, Loss: 3.743908\
Failed-2 (Overfit) Epoch 8: Train Acc: 12.60%, Test Acc: 10.10%, Loss: 2.495821\
Failed-2 (Overfit) Epoch 9, Batch 0, Loss: 2.297558\
Failed-2 (Overfit) Epoch 9: Train Acc: 10.40%, Test Acc: 10.09%, Loss: 2.304707\
Failed-2 (Overfit) Epoch 10, Batch 0, Loss: 2.308132\
Failed-2 (Overfit) Epoch 10: Train Acc: 8.20%, Test Acc: 10.09%, Loss: 2.304872\
==================================================\
FAILED EXPERIMENT 3: TERRIBLE ARCHITECTURE\
==================================================\
Using huge kernels, aggressive pooling, extreme dropout, and bottleneck layers\
Failed-3 (Bad Arch) Epoch 1, Batch 0, Loss: 2.468977\
Failed-3 (Bad Arch) Epoch 1, Batch 200, Loss: 2.613589\
Failed-3 (Bad Arch) Epoch 1, Batch 400, Loss: 2.354839\
Failed-3 (Bad Arch) Epoch 1, Batch 600, Loss: 2.312117\
Failed-3 (Bad Arch) Epoch 1, Batch 800, Loss: 2.287613\
Failed-3 (Bad Arch) Epoch 1: Train Acc: 10.15%, Test Acc: 11.36%, Loss: 2.368794\
Failed-3 (Bad Arch) Epoch 2, Batch 0, Loss: 2.315654\
Failed-3 (Bad Arch) Epoch 2, Batch 200, Loss: 2.328147\
Failed-3 (Bad Arch) Epoch 2, Batch 400, Loss: 2.295503\
Failed-3 (Bad Arch) Epoch 2, Batch 600, Loss: 2.303617\
Failed-3 (Bad Arch) Epoch 2, Batch 800, Loss: 2.377260\
Failed-3 (Bad Arch) Epoch 2: Train Acc: 11.23%, Test Acc: 11.35%, Loss: 2.324148\
Failed-3 (Bad Arch) Epoch 3, Batch 0, Loss: 2.300964\
Failed-3 (Bad Arch) Epoch 3, Batch 200, Loss: 2.299092\
Failed-3 (Bad Arch) Epoch 3, Batch 400, Loss: 2.310454\
Failed-3 (Bad Arch) Epoch 3, Batch 600, Loss: 2.291498\
Failed-3 (Bad Arch) Epoch 3, Batch 800, Loss: 2.294657\
Failed-3 (Bad Arch) Epoch 3: Train Acc: 11.28%, Test Acc: 11.35%, Loss: 2.304453\
Failed-3 (Bad Arch) Epoch 4, Batch 0, Loss: 2.315679\
Failed-3 (Bad Arch) Epoch 4, Batch 200, Loss: 2.298822\
Failed-3 (Bad Arch) Epoch 4, Batch 400, Loss: 2.310490\
Failed-3 (Bad Arch) Epoch 4, Batch 600, Loss: 2.306109\
Failed-3 (Bad Arch) Epoch 4, Batch 800, Loss: 2.298993\
Failed-3 (Bad Arch) Epoch 4: Train Acc: 11.30%, Test Acc: 11.35%, Loss: 2.300887\
Failed-3 (Bad Arch) Epoch 5, Batch 0, Loss: 2.303672\
Failed-3 (Bad Arch) Epoch 5, Batch 200, Loss: 2.295135\
Failed-3 (Bad Arch) Epoch 5, Batch 400, Loss: 2.290288\
Failed-3 (Bad Arch) Epoch 5, Batch 600, Loss: 2.287966\
Failed-3 (Bad Arch) Epoch 5, Batch 800, Loss: 2.279579\
Failed-3 (Bad Arch) Epoch 5: Train Acc: 11.31%, Test Acc: 11.35%, Loss: 2.300349\
Failed-3 (Bad Arch) Epoch 6, Batch 0, Loss: 2.305222\
Failed-3 (Bad Arch) Epoch 6, Batch 200, Loss: 2.301864\
Failed-3 (Bad Arch) Epoch 6, Batch 400, Loss: 2.292328\
Failed-3 (Bad Arch) Epoch 6, Batch 600, Loss: 2.290645\
Failed-3 (Bad Arch) Epoch 6, Batch 800, Loss: 2.324824\
Failed-3 (Bad Arch) Epoch 6: Train Acc: 11.33%, Test Acc: 11.35%, Loss: 2.299103\
Failed-3 (Bad Arch) Epoch 7, Batch 0, Loss: 2.314896\
Failed-3 (Bad Arch) Epoch 7, Batch 200, Loss: 2.288831\
Failed-3 (Bad Arch) Epoch 7, Batch 400, Loss: 2.310268\
Failed-3 (Bad Arch) Epoch 7, Batch 600, Loss: 2.332127\
Failed-3 (Bad Arch) Epoch 7, Batch 800, Loss: 2.290131\
Failed-3 (Bad Arch) Epoch 7: Train Acc: 11.33%, Test Acc: 19.70%, Loss: 2.298903\
Failed-3 (Bad Arch) Epoch 8, Batch 0, Loss: 2.283385\
Failed-3 (Bad Arch) Epoch 8, Batch 200, Loss: 2.296928\
Failed-3 (Bad Arch) Epoch 8, Batch 400, Loss: 2.304867\
Failed-3 (Bad Arch) Epoch 8, Batch 600, Loss: 2.289958\
Failed-3 (Bad Arch) Epoch 8, Batch 800, Loss: 2.285449\
Failed-3 (Bad Arch) Epoch 8: Train Acc: 11.32%, Test Acc: 20.80%, Loss: 2.297694\
Failed-3 (Bad Arch) Epoch 9, Batch 0, Loss: 2.317934\
Failed-3 (Bad Arch) Epoch 9, Batch 200, Loss: 2.320561\
Failed-3 (Bad Arch) Epoch 9, Batch 400, Loss: 2.286733\
Failed-3 (Bad Arch) Epoch 9, Batch 600, Loss: 2.292740\
Failed-3 (Bad Arch) Epoch 9, Batch 800, Loss: 2.300556\
Failed-3 (Bad Arch) Epoch 9: Train Acc: 11.36%, Test Acc: 18.88%, Loss: 2.296804\
Failed-3 (Bad Arch) Epoch 10, Batch 0, Loss: 2.303607\
Failed-3 (Bad Arch) Epoch 10, Batch 200, Loss: 2.297061\
Failed-3 (Bad Arch) Epoch 10, Batch 400, Loss: 2.299744\
Failed-3 (Bad Arch) Epoch 10, Batch 600, Loss: 2.278504\
Failed-3 (Bad Arch) Epoch 10, Batch 800, Loss: 2.288708\
Failed-3 (Bad Arch) Epoch 10: Train Acc: 11.33%, Test Acc: 19.25%, Loss: 2.296321\
\
================================================================================\
DETAILED FAILURE ANALYSIS\
================================================================================\
\
1. FAILED EXPERIMENT 1: EXTREME LEARNING RATE (LR = 10.0)\
------------------------------------------------------------\
WHY IT FAILED:\
\'95 Learning rate of 10.0 is extremely high for neural networks\
\'95 Causes gradient explosion - weights update too aggressively\
\'95 Network parameters oscillate wildly and never converge\
\'95 Loss may increase instead of decrease\
\'95 Accuracy remains at random guessing level (~10% for 10-class problem)\
\
EXPECTED BEHAVIOR:\
\'95 Training loss will be erratic and high\
\'95 Accuracy will hover around 10% (random guessing)\
\'95 Gradients will explode, causing numerical instability\
\
2. FAILED EXPERIMENT 2: MASSIVE OVERPARAMETERIZATION\
------------------------------------------------------------\
WHY IT FAILED:\
\'95 Network has millions of parameters but only 500 training samples\
\'95 Severe overfitting - memorizes training data perfectly\
\'95 No generalization to test data\
\'95 Model complexity >> data complexity\
\'95 No regularization (dropout removed intentionally)\
\
EXPECTED BEHAVIOR:\
\'95 Training accuracy reaches near 100%\
\'95 Test accuracy remains very low (poor generalization)\
\'95 Huge gap between train and test performance\
\
3. FAILED EXPERIMENT 3: TERRIBLE ARCHITECTURE\
------------------------------------------------------------\
WHY IT FAILED:\
\'95 Huge kernels (15x15, 13x13) destroy spatial relationships\
\'95 Aggressive pooling (5x5) loses critical information\
\'95 Extreme dropout (95%) randomly zeros most neurons\
\'95 Bottleneck layer (2 neurons) creates information bottleneck\
\'95 Tanh activation can saturate and kill gradients\
\
EXPECTED BEHAVIOR:\
\'95 Poor feature extraction due to bad conv layers\
\'95 Information loss from aggressive pooling\
\'95 Training instability from extreme dropout\
\'95 Low accuracy due to insufficient representational capacity\
\
================================================================================\
FINAL PERFORMANCE SUMMARY\
================================================================================\
Baseline                  | Final Test Accuracy:  99.33% | Final Loss: 0.021740\
Failed Experiment 1       | Final Test Accuracy:   8.92% | Final Loss: 2.931625\
Failed Experiment 2       | Final Test Accuracy:  10.09% | Final Loss: 2.304872\
Failed Experiment 3       | Final Test Accuracy:  19.25% | Final Loss: 2.296321}